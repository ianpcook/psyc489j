<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

Stan
========================================================
author: Jeffrey Chrabaszcz
date: 9 January 2014
transition: none

```{r setup, include=FALSE, echo=FALSE}
require(knitr)
require(ggplot2)
opts_chunk$set(include=TRUE,cache=TRUE,fig.align='center')
options(digits = 3)
```

Outline
========================================================

1. Statistical philosophy
2. Bayesian inference
2. Likelihood calculation for continuous variables
3. Maximizing likelihood
4. Markov chains

Statistical philosophy
========================================================
left: 50%

Frequentist
---

* Pros
  * intuitive to run
  * influential
  * ubiquitous defaults
* Cons
  * limited application
  * abused
  * strange to interpret

***

Bayesian
---

* Pros
  * flexible
  * easy interpretation
  * anwers the right question!
* Cons
  * complex computation
  * slow adoption

Bayesian inference
========================================================

In Bayesian inference, we generally consider:

* what we already know (priors)
* what we learn from our current observation (likelihood)
* how much our beliefs should change as a result of our observation (posterior)

Prior
========================================================

This should represent everything we know about our problem before we start.

* scale boundaries
* previous research
* related observations

Often a range of priors are appropriate.
Rarely is a uniform prior appropriate.

Likelihood
========================================================

Assuming no other information, we can maximize the likelihood of the data by maximizing the objective function.

$$
\hat{\ell}(\theta|x) = \frac{1}{n}\sum_{i = 1}^n \ln f(x_i|\theta)
$$

This is what we've already been doing with `lm()`!

Posterior
========================================================

The hard part about Bayesian statistics is combining the prior and likelihood functions to determine the posterior distribution.
Done correctly, this yield an intuitive interpretation: what is the probability of a given parameter value.

```{r pre_bayes,echo=FALSE,fig.width=14,fig.height=6}
x <- seq(0, 1, by = .01)
a <- data.frame(x = rep(x, 3), y = c(dbeta(x, 1, 1), dbeta(x, 1, 5), dbeta(x, 2, 6)), rate = factor(rep(1:3, each = length(x)), labels = c("prior", "likelihood", "posterior")))
qplot(x, y, data = a, geom = "line") + facet_grid(.~rate) + theme_bw(base_size=24)
```

Data!
========================================================

How can you have a probability of something happen if you know it's happened?

$$
P(D|H1) + P(D|\neg H1)
$$

$$
\sum_{i=1}^{k} P(D|H_i)
$$

This is only letting us interpret things in direct probabilities rather than unnormalized ratios.

Bayesian cancer updating
========================================================

* 1% of women have breast cancer
* 80% of mammograms detect breast cancer given that it is present
* 90.4% of mammograms correctly return a negative result given that breast cancer is not present

Question
---

How should we react to a negative test?

========================================================

We know that the probability of a union is just the product of both probabilities, so:

.        | C        | ~C        
---------|----------|-----------
Positive | .01 * .8 | .99 * .096
Negative | .01 * .2 | .99 * .904

So we can solve this:

$$
P(C|Pos) = \frac{P(C \cap Pos)}{P(Pos)} = \frac{.01 \cdot .8}{(.01 \cdot .8 + .99 \cdot .096)}
$$

```{r cancer_calc}
.01 * .8/(.01 * .8 + .99 * .096)
```

Likelihood calculation for continuous variable
========================================================

The normal distribution is defined for all possible values of X.
Say we sample five values from a standard normal distribution.

```{r rand_print,echo=FALSE}
n <- 5
dat <- rnorm(n)
dat
```

We can visualize the probability density associated with each of these samples assuming $\mu = 0$ and $\sigma = 1$ on the following graph.

```{r lik_plot,echo=FALSE,fig.height=4,fig.width=10}
df <- data.frame(x = rep(dat,2), y = c(dnorm(dat), rep(0, n)), groups = rep(1:n, 2))
df.2 <- data.frame(x = seq(-5, 5, by = .01), y = dnorm(seq(-5, 5, by = .01)))
ggplot(df, aes(x = x, y = y, group = groups)) + geom_line(color = "firebrick") + geom_line(data = df.2, aes(x = x, y = y, group = 1), color = "black") + theme_grey(base_size = 24) + xlim(c(-5, 5))
```

Heights of lines
========================================================

The heights of those lines are dictated by the parameters of the normal distribution.
That means we can change the heights by changing the parameters!

```{r lik_plot2,echo=FALSE,fig.height=7,fig.width=14}
df <- data.frame(x = rep(dat,4), y = c(dnorm(dat), rep(0, n), dnorm(dat, mean = 1), rep(0, n)), groups = rep(1:n, 4), mu = rep(0:1, each = 2*n))
df.2 <- data.frame(x = rep(seq(-5, 5, by = .01), 2), y = c(dnorm(seq(-5, 5, by = .01)), dnorm(seq(-5, 5, by = .01), mean = 1)), mu = rep(0:1, each = length(seq(-5, 5, by = .01))))
ggplot(df, aes(x = x, y = y, group = groups)) + geom_line(color = "firebrick") + geom_line(data = df.2, aes(x = x, y = y, group = 1), color = "black") + theme_grey(base_size = 24) + facet_wrap(~mu) + xlim(c(-5, 5))
```

Maximizing likelihood
========================================================

```{r}
logL <- function(coefs, y, x, sdr) {
  means <- coefs[1] + coefs[2] * x
  dat <- cbind(y, means, sdr)
  ll <- apply(dat, 1, function(x) dnorm(x[1], mean = x[2], sd = x[3]))
  return(sum(log(ll)))
}
bootS <- function(model.formula, df) {
  new.df <- df[sample(nrow(df), nrow(df), replace = TRUE),]
  mod <- lm(model.formula, data = new.df)
  return(coef(mod))
}
getRSD <- function(coefs, y, x) {
  y.hat <- coefs[1] + coefs[2] * x
  resids <- y - y.hat
  return(sd(resids))
}
```

Outputs for ML (lm vs. bootstrap)
========================================================

```{r,echo=FALSE}
options(digits = 5)
```

```{r}
data(women)
fit <- lm(weight ~ height, data = women)
fit.ll <- logL(coef(fit), women$weight, women$height, sd(residuals(fit)))
b <- t(replicate(100, bootS(weight ~ height, women)))
rsds <- apply(b, 1, function(x) getRSD(x, women$weight, women$height))
a <- cbind(b, rsds)
all.lls <- apply(a, 1, function(x) logL(x[1:2], women$weight, women$height, x[3]))
c(fit.ll, max(all.lls))
```

```{r,echo=FALSE}
options(digits = 3)
```

Random search
========================================================

A class of optimization methods that can be used with problems that are not continuous or differentiable.
Encompasses two general approaches.

Point estimation
---

Find a maximum/minimum point for some outcome.
A lot of you probably think of information criteria in this way.

Distribution estimation
---

A sampling procedure that converges to a distribution rather than a single point.
This is what we'll be doing.

Quick point-estimation example
========================================================

Say we have a normal distribution and need to find the x value for which f(x) is maximized.
We'll set up a toy example for which the true maximized value is 0.

```{r}
xs <- runif(10000, -3, 10)
xs[dnorm(xs) == max(dnorm(xs))]
```

This method is really only useful in very low-dimensional settings, but it's a very simple demonstration of how random search could work.

Markov chains
========================================================

Kruschke's island-hopping example is the clearest explanation I've ever heard for this.

========================================================

<pre><code style="font-size:18pt">
islands <- 1:3
visits <- rep(NA, times = 100000)
visits[1] <- sample(islands, 1)
for (i in 1:(length(visits) - 1)) {
  if (visits[i] == 1) {
    choose.direction <- sample(c(0, 1), 1)
  } else if (visits[i] == 6) {
    choose.direction <- sample(c(-1, 0), 1)
  } else {
    choose.direction <- sample(c(-1, 1), 1)
  }
  if (islands[visits[i]] < islands[visits[i] + choose.direction]) {
    visits[i + 1] <- visits[i] + choose.direction
  } else if (runif(1) < islands[visits[i] + choose.direction]/islands[visits[i]]) {
    visits[i + 1] <- islands[visits[i]] + choose.direction
  } else {
    visits[i + 1] <- visits[i]
  }
}
</code></pre>

```{r islandcode,results='hide',echo=FALSE}
islands <- 1:6
visits <- rep(NA, times = 100000)
visits[1] <- sample(islands, 1)
for (i in 1:(length(visits) - 1)) {
  if (visits[i] == 1) {
    choose.direction <- sample(c(0, 1), 1)
  } else if (visits[i] == 6) {
    choose.direction <- sample(c(-1, 0), 1)
  } else {
    choose.direction <- sample(c(-1, 1), 1)
  }
  if (islands[visits[i]] < islands[visits[i] + choose.direction]) {
    visits[i + 1] <- visits[i] + choose.direction
  } else if (runif(1) < islands[visits[i] + choose.direction]/islands[visits[i]]) {
    visits[i + 1] <- islands[visits[i]] + choose.direction
  } else {
    visits[i + 1] <- visits[i]
  }
}
```
Island example output
========================================================

```{r run_mc}
mcmc.approx <- table(visits)/length(visits)
true.props <- islands/sum(islands)
names(true.props) <- names(mcmc.approx)

mcmc.approx
true.props
```

Stan/HMC
========================================================

Stan is a piece of software developed to implement Hamiltonian Markov Chain simulations.
Stan can be controlled from R, this solves a lot of problems with data management and output inspection.
Unfortunately, as with JAGS, Stan has a unique syntax for building models.
This syntax comes in four sections:

* Data
* Parameters
* Transformed Parameters
* Model

Data
========================================================

```{r,eval=FALSE}
data {
  int<lower=0> N;
  int<lower=0> K;
  vector[N] y;
  matrix[N,K] x;
}
```

Parameters
========================================================

```{r,eval=FALSE}
parameters {
  vector[K] beta;
  real<lower=0> sigma;
}
```

Model
========================================================

```{r,eval=FALSE}
model {
  y ~ normal(x * beta, sigma);
}
```

Passing data to stan
========================================================

```{r,eval=FALSE}

x <- model.matrix(form, data)

stan_dat <- list(
  y = model.frame(form, data)[,1],
  x = x,
  N = nrow(x),
  K = ncol(x)
)
```

Stan setup
========================================================

```{r full_code,echo=FALSE,reslts='hide'}
library(rstan)
library(ISwR)
data(juul)
juul <- juul[complete.cases(juul$sex, juul$igf1),]
source("~/Dropbox/psyc489j_2015/code/stan_lm.R")
fit <- stanLM(igf1 ~ sex, data = juul, refresh = -1)
```

```{r full_code_print,eval=FALSE}
library(rstan)
library(ISwR)
data(juul)
juul <- juul[complete.cases(juul$sex, juul$igf1),]
fit <- stanLM(igf1 ~ sex, data = juul)
```

lm output
========================================================

```{r}
library(arm)
display(lm(igf1 ~ sex, data = juul))
```

rstan output
========================================================

<pre><code style="font-size:20pt">
```{r}
print(summary(fit, probs = c(.025, .975))$summary, digits = 1)
```
</code></pre>