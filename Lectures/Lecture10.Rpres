<script type="text/javascript"
       src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
   tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
 });
</script>

Bayesian estimation supercedes the t test
========================================================
author: Jeffrey Chrabaszcz
date: 14 January 2014
transition: none
width: 1024
height: 760

```{r setup, include=FALSE, echo=FALSE}
require(knitr)
require(ggplot2)
opts_chunk$set(include=TRUE,cache=TRUE,fig.align='center')
options(digits = 3)
```

Work/time relationship
========================================================

<center>
![programming](http://cdn.oreilly.com/radar/images/posts/data-jujitsu/2-work-vs-time-580.png)
</center>

Outline
========================================================

1. Thinking about distributions
2. Exponential distribution
3. Hyperparameters
4. Beyond means
5. Priors for practical purposes
6. BEST

Exponential distribution
========================================================

The exponential distribution is only defined for $x \ge 0$.

$$
f(x) = \lambda e^{-\lambda x}
$$

```{r exp_plot,echo=FALSE}
x <- seq(0, 200, by = .01)
qplot(x, dexp(x, 1/29), geom = "line") + theme_grey(base_size = 24)
```

Thinking about distributions
========================================================

When using MCMC methods, you start to focus on different properties of distributions.

* Boundedness - single bound, double-bound, or unbound?
* Flexibility - beta can look like anything, uniform is nearly invariant
* Generation speed - generating draws from $\mathcal{N}(0, 1)$ is trivial, Dirichlet and Cauchy draws are not

Priors for practical purposes
========================================================

Priors are included for two reasons:

1. Encode information relevant to a problem
2. Speed model convergence

When N becomes large, most things are approximated with a normal distribution.
Before that, it often makes sense to pick a more appropriate likelihood distribution, especially when scores are bounded or priors are highly informed.

Hyperparameters
========================================================

We're going to abuse the word **prior** in a short few slides.

$$
y_i \sim \mathcal{N}(\beta_0 + \beta_k X_{ki}, \epsilon_i^2)
$$

By default, we assume that our coefficients are drawn from a uniform without bounds.

$$
\beta_k \sim \mathcal{U}(-\infty, \infty)
$$

We can commonly improve prediction by instead using LASSO.

$$
\beta_k \sim \mathcal{N}(0, \sigma_y^2)
$$

Beyond means
========================================================
left: 50%

Other parameters
---

What happens when we want to test the difference between the means of groups that have different variances?
How about testing the difference between variances?
The F test and Barlett's test both accomplish this, but are sensitive to violations of normality.

***

Other distributions
---

What we don't want to assume a normal distribution and don't have a link function, or don't know the link function?

MCMC sampling offers a way to estimate distributions are parameters of any specified distribution.

BEST
========================================================

We can run a Bayesian t-test with BEST.

```{r gen_data}
library(BEST)
library(ISwR)
data(juul)
juul <- juul[complete.cases(juul$sex, juul$igf1),]
mod <- BESTmcmc(juul$igf1[juul$sex == 1], juul$igf1[juul$sex == 2], verbose = FALSE)
```

BEST object
========================================================

<pre><code style="font-size:18pt">
```{r,results='asis',echo=FALSE}
str(mod)
```
</code></pre>

BEST output
========================================================

<pre><code style="font-size:18pt">
```{r,results='asis',echo=FALSE}
summary(mod)
```
</code></pre>