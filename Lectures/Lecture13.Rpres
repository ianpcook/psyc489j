<script type="text/javascript"
       src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
   tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
 });
</script>

Multilevel models
========================================================
author: Jeffrey Chrabaszcz
date: 21 January 2014
transition: none
width: 1024
height: 760

```{r setup,include=FALSE,echo=FALSE}
require(knitr)
require(ggplot2)
library(arm)
opts_chunk$set(include=TRUE,cache=TRUE,fig.align='center')
options(digits = 3)
```

Outline
========================================================

1. Why use multilevel models?
2. Varying intercepts and slopes
3. Partial pooling without predictors
4. Partial pooling with predictors
5. Quickly fitting multilevel models in R
6. Five ways to write the same model

Why use multilevel models?
========================================================

We say that observations are independent if:

$$
P(A \cap B) = P(A) \cdot P(B)
$$

All of our statistical tests assume independence of our data, you can tell by looking at the likelihood formula!

$$
\hat{\ell}(\theta|x) = \frac{1}{n}\sum_{i = 1}^n \ln f(x_i|\theta)
$$

When we violate independence, our tests are necessarily overconfident, (error bars are too narrow).
This happens when there is structure in the data not reflected in our tests.

Varying intercepts and slopes
========================================================

With MLM, we introduce information into the model relevant to known sources of dependence.
We can then allow outcomes levels, or relationships between the outcome and our predictors, to vary as a function of these sources.

```{r intercept_slope_plot,echo=FALSE,fig.height=4,fig.width=12}
df <- data.frame(intercept = c(1, 1, 1, 1, 1, 1.5, 2, .5, 1, 1.5, 2, .5), slope = c(1, 2, .5, 1.5, 1, 1, 1, 1, 1, 2, .5, 1.5), subject = factor(rep(1:4, times = 3)), example = factor(rep(c("random slopes", "random intercepts", "random intercepts and slopes"), each = 4), ordered = TRUE))
df.points <- data.frame(x = rep(-1:2, times = 3), y = rep(0:3, times = 3), example = factor(rep(c("random slopes", "random intercepts", "random intercepts and slopes"), each = 4), ordered = TRUE))
df$mean.intercept <- rep(c(1, 1.25, 1.25), each = 4)
df$mean.slope <- rep(c(1.25, 1, 1.25), each = 4)
p <- ggplot(df.points, aes(x = x, y = y)) + geom_point(size = 0) + facet_wrap(~example)
p + geom_abline(data = df, aes(intercept = intercept, slope = slope), color = "black", size = 1.5, alpha = 1/2) + geom_abline(data = df, aes(intercept = mean.intercept, slope = mean.slope), color = "firebrick", size = 2) + facet_wrap(~example) + theme_bw(base_size = 24)
```

Critically, we do this in a way that still informs our predictors across all groups.

Partial pooling
========================================================

We say multilevel estimates are partially pooled because they represent a compromise between full pooling and no pooling.

* Full pooling - assume each group is independent of the others.
* No pooling - assume the groups do not differ in any meaningful way.
* Partial pooling - assume groups are drawn from a shared distribution.

No pooling and full pooling both be implemented in **lm()**.
Partial pooling requires some extra machinery.

Partial pooling
========================================================

To get partial pooling estimates, we have to add to the linear model.
We know the linear model as

$$
y_i = \beta_0 + \beta_1 \cdot x_{1i} + \epsilon_i
$$

We can now add one subscript and a second equation to describe random intercepts.

$$
\begin{align}
y_i &= \beta_{0[j]} + \beta_1 \cdot x_{1i} + \epsilon_i \\
\beta_{0[j]} &= \gamma_0 \cdot u_1 + \epsilon_j
\end{align}
$$

Partial pooling
========================================================

We've already looked at stan, so the following more general equations might make more sense.

$$
\begin{align}
y_i &\sim \mathcal{N}(X_i\beta_{j}, \sigma_i^2) \\
\beta_j &\sim \mathcal{N}(U_j\gamma, \sigma_j^2)
\end{align}
$$

This should look something like code we've already encountered:

```{r,eval=FALSE}
model {
  y[i] ~ normal(inprod(x[i], b), sigma);
  b ~ normal(0, 1);
}
```

Now we're just going to add observed information into that hierarchical level.

Partial pooling without predictors
========================================================

We've looked at the sleep data before, we can use it to examine how partial pooling works.
First, let's Look at full pooling and no pooling estimates.

```{r pool_comp}
data(sleep)
pool <- lm(extra ~ 1, data = sleep)
no.pool <- lm(extra ~ ID, data = sleep)
library(lme4)
part.pool <- lmer(extra ~ 1 + (1|ID), data = sleep)
```

Full pooling
========================================================

```{r}
display(pool)
```

No pooling
========================================================

```{r}
display(no.pool)
```

Partial pooling
========================================================

```{r}
display(part.pool)
```

Graphical comparison
========================================================

```{r pool_boxes,echo=FALSE,fig.height=4,fig.width=10}
df <- data.frame(x = factor(c(rep(1, length(sleep$extra)), sleep$ID)), y = rep(sleep$extra, times = 2), pool = factor(rep(c("full", "none"), each = length(sleep$extra))))
ggplot(df, aes(x, y)) + geom_boxplot() + facet_wrap(~pool, drop = TRUE, scales = "free_x") + theme_bw(base_size = 24)
```

```{r model_coefs,echo=FALSE,fig.height=4,fig.width=10}
df <- data.frame(x = factor(c(1, rep(unique(sleep$ID), times = 2))), y = c(coef(pool), c(coef(no.pool)[1], coef(no.pool)[-1] + coef(no.pool)[1]), coef(part.pool)$ID[,1]), pool = factor(rep(c("full", "none", "partial"), c(1, 10, 10))), se = c(summary(pool)$coef[,2], coef(summary(no.pool))[,2], se.ranef(part.pool)$ID))
ggplot(df, aes(x, y, ymin = y - 2 * se, ymax = y + 2 * se)) + geom_point() + geom_errorbar(width = 0) + facet_wrap(~pool, drop = TRUE, scales = "free_x") + theme_bw(base_size = 24)
```

Partial pooling with predictors
========================================================

```{r pred_setup,echo=FALSE}
pool <- lm(extra ~ group, data = sleep)
no.pool <- lm(extra ~ group + ID, data = sleep)
part.pool <- lmer(extra ~ group + (1|ID), data = sleep)
```

```{r}
display(pool)
```

Partial pooling with predictors
========================================================

```{r}
display(no.pool)
```

Partial pooling with predictors
========================================================

```{r}
display(part.pool)
```

Graphical analogue
========================================================

```{r}
ggplot(sleep, aes(group, extra)) + geom_boxplot(group = 1) + geom_jitter(size = 5, position = position_jitter(.25), aes(color = ID)) + theme_bw(base_size = 24)
```

Multilevel model syntax in lme4
========================================================

**lme4** is by major package used to fit multilevel models in R.
The syntax is very similar to the **lm** function.

```
lmer(outcome ~ predictor + (random intercept and/or slope | group), data = df, ...)
```

For exampe, imagine we had multiple measurements from each person for both groups and wanted to see if the difference varied by person.
We could fit:

```{r,eval=FALSE}
fit <- lmer(extra ~ group + (1 + group|ID), data = sleep)
```

Five ways to write the same model
========================================================

There are at least five equivalent ways to think of a multilevel model.

1. Allowing regression coefficients to vary across groups
2. Combining separate local regressions
3. Modeling the coefficients of a large regression model
4. Regression with multiple error terms
5. Large regression with correlated errors

We'll use a simple version of the linear model as a base:

$$
y_i = \alpha + \beta \cdot x_i + \epsilon_i
$$

Allowing regression coefficients to vary across groups
========================================================

$$
\begin{align}
  y_i &= \alpha_{j[i]} + \beta \cdot x_i + \epsilon_i \\
  \alpha_j &= \mu_{\alpha} + \eta_{j}
\end{align}
$$

In this case we have two "hidden" distributional assumptions.

$$
\begin{align}
  y_i &\sim \mathcal{N}(\alpha_j + \beta x_i, \sigma_y^2) \\
  \alpha_i &\sim \mathcal{N}(\mu_\alpha, \sigma^2_{\alpha})
\end{align}
$$

Combining separate local regressions
========================================================

Within each group:

$$
y_i \sim \mathcal{N}(\alpha_j + \beta x_i, \sigma^2_y)
$$

While the intercept term, the thing that varies by group, is:

$$
\alpha_j \sim \mathcal{N}(\gamma_0 + \gamma_1 u_j, \sigma^2_{\alpha})
$$

Modeling the coefficients of a large regression model
========================================================

Here we that the coefficients are drawn from a distribution 

$$
\begin{align}
  y_i &\sim \mathcal{N}(X_i \beta, \sigma^2_y) \\
  \beta_j &\sim \mathcal{N}(0, \sigma^2_{\alpha})
\end{align}
$$

Remember that we have an intercept coded in our X matrix.
The coefficients are centered at zero, but the y-intercept emerges when all other X values are zeroed.

Regression with multiple error terms
========================================================

Here we specify a shift in the mean based on a second error distribution.

$$
\begin{align}
  y_i &\sim \mathcal{N}(X_i \beta + \eta_{j[i]}, \sigma^2_y) \\
  \eta_j &\sim{N}(0, \sigma^2_{\alpha})
\end{align}
$$

Large regression with correlated errors
========================================================

Here we say that the variance parameter for the error distribution varies by group membership.

$$
\begin{align}
  y_i &= X_i \beta + \epsilon^{all} \\
  \epsilon^{all} &\sim \mathcal{N}(0, \Sigma)
\end{align}
$$