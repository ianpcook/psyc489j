<script type="text/javascript"
       src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
   tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
 });
</script>

Linear regression: the basics
========================================================
author: Jeffrey Chrabaszcz
date: 7 January 2014
transition: none
width: 1024
height: 760

```{r setup, include=FALSE, echo=FALSE}
require(knitr)
require(arm)
require(ggplot2)
opts_chunk$set(include=TRUE,cache=TRUE,fig.align='center')
options(digits = 3)
```

Data science
========================================================

<center>![data_science](https://lh3.googleusercontent.com/2fSoP_UUWS6okE7JDI-bWa6xQJ3-ylcd7WGrxGU8UQjNykD_2wLKOB-sgt_LYq2ZZB9xlBMj30il4rSmUyqEyqy4uoFy4iDTly09CnRrB2PbEcIDGZ3d)</center>

Outline
========================================================

1. Linear model
2. Multiple predictors
3. Interactions
4. Statistical inference
5. Graphical displays of data and fitted model
6. Assumptions and diagnostics
7. Prediction and validation

Linear model
========================================================

Recall that the linear model is:

$$
y_i = \beta_0 + \beta_1x_{1i} + \beta_2x_{2i} + ... + \epsilon_i\\
\hat{y} = \beta_0 + \beta_1x_{1i} + ... + \beta_kx_{ki}
$$

This means that, for some sample, our best prediction of each value of $y$ is:

* $\beta_0$, the intercept term; plus,
* the weighted value of each predictor, $x$, multiplied by some weight, $\beta$.
* $\epsilon$ also gives us an idea of how for off we expect to be with each prediction.

Quick aside
========================================================

The data for this next bit were originally formatted for Stata, (another statistical analysis software).
This is what it took to get those data into R.

```{r iq_data}
library(foreign)
df <- read.dta("data/kidiq.dta")
head(df)
```

Simple linear regression
========================================================
left: 50%

Let's say we want to predict a child's IQ based on whether or not the mother completed highschool.
We can look at this relationship graphically.

```{r simple_regression,eval=FALSE}
df$mom_hs <- factor(df$mom_hs)
qplot(mom_hs, kid_score, data = df, geom = "jitter", position = position_jitter(width = .25))
```

***

```{r simple_regression2,echo=FALSE}
df$mom_hs <- factor(df$mom_hs)
qplot(mom_hs, kid_score, data = df, geom = "jitter", position = position_jitter(width = .25)) + theme_bw(base_size = 24)
```

Alternately, boxplots for a single categorical predictor
========================================================

```{r iq_measles,fig.height=6,fig.width=10}
ggplot(df, aes(mom_hs, kid_score)) + geom_boxplot(width = .6) + geom_jitter(color = "firebrick", alpha = .6, position = position_jitter(width = .25)) + theme_bw(base_size = 24)
```

t test
========================================================

```{r t_test}
t.test(kid_score ~ mom_hs, data = df, var.equal = TRUE)
```

Code you will use a million times
========================================================

This is the code we'll use to predict kid's IQ based on mom's HS completion.
We assigned our data to the data.frame named **df**, so that's also specified.

```{r first_lm}
my.lm <- lm(kid_score ~ mom_hs, data = df)
```

This is just here for reference, you'll use this code many times in the next 2 weeks.

Display a linear model
========================================================

$$
\widehat{kid\,score} = 78 + 12 \cdot mom\,hs
$$

```{r display_lm}
display(my.lm)
```

Summary of a linear model
========================================================

<pre><code style="font-size:20pt">
```{r summary_lm,echo=FALSE,results="asis"}
print(summary(my.lm), signif.stars = FALSE, call = FALSE)
```
</code></pre>

Continuous predictor
========================================================

$$
\widehat{kid\,score} = 26 + 0.6 \cdot mom\,IQ
$$

```{r continuous_lm,echo=FALSE}
display(my.lm1 <- lm(kid_score ~ mom_iq, data = df))
```

Connecting marginal means
========================================================

```{r t_test_slope_data,echo=FALSE}
a <- data.frame(x = 0:1, means = c(coef(my.lm)[1], sum(coef(my.lm))), ses = summary(my.lm)$coef[,2])
```

```{r t_test_plot}
ggplot(a, aes(x, means)) + geom_point() + geom_line(group = 1) + geom_errorbar(aes(ymin = means - ses, ymax = means + ses), width = .2)
```

Connecting marginal means
========================================================

```{r marginal_means,echo=FALSE,fig.height=9,fig.width=10}
qplot(mom_iq, kid_score, data = df) + geom_smooth(se = FALSE, method = "lm", size = 1.5) + theme_bw(base_size = 24)
```

Naming conventions
========================================================

I will not be using any of the following names in this class, but early statisticians did not have the benefit of hindsight.

Name                |   Outcome   | Predictor(s)
--------------------|-------------|-----------------------------------
t-test              | continuous  | one, dichotomous
one-way ANOVA       | continuous  | one, categorical
2x2 ANOVA           | continuous  | three! categorical
ANCOVA              | continuous  | one categorical and one continuous
correlation         | continuous  | one, continuous
multiple regression | continuous  | one or more, any kind
logistic regression | dichotomous | one or more, any kind
poisson regression  | categorical | one or more, any kind

Multiple predictors
========================================================

$$
\widehat{kid\,score} = 26 + 6 \cdot mom\,hs + 0.6 \cdot mom\,IQ
$$

```{r multiple_predictors,echo=FALSE}
my.lm2 <- lm(kid_score ~ mom_hs + mom_iq, data = df)
```

```{r}
display(my.lm2)
```

Visualizing multiple predictors
========================================================

Heavily dependent on the nature of your predictors!

```{r mult_pred,echo=FALSE,fig.height=8,fig.width=11}
df$intercept <- ifelse(unclass(df$mom_hs) == 1, coef(my.lm2)[1], sum(coef(my.lm2)[1:2]))
df$slope <- coef(my.lm2)[3]
qplot(mom_iq, kid_score, data = df, color = mom_hs, alpha = .6) + geom_abline(aes(intercept = intercept, slope = slope, color = mom_hs)) + scale_color_manual(values = c("firebrick", "steelblue")) + theme_bw(base_size = 24) + guides(alpha = FALSE)
```

Interactions
========================================================

$$
\begin{align}
    \widehat{kid\,score} = 11 &+ 51 \cdot mom\,hs \\
    & +\;1.1 \cdot mom\,IQ \\
    & -\;0.5 \cdot mom\,hs \cdot mom\,IQ
\end{align}
$$

```{r inter_mod,echo=FALSE}
display(my.lm3 <- lm(kid_score ~ mom_hs * mom_iq, data = df))
```

Plotting interactions
========================================================

```{r int_viz,echo=FALSE,fig.height=9,fig.width=10}
p <- ggplot(df, aes(x = mom_iq, y = kid_score, color = mom_hs)) + geom_point(alpha = .5) + scale_color_manual(values = c("firebrick", "steelblue")) + theme_bw(base_size = 24)
p + geom_smooth(se = FALSE, method = "lm")
```

Plotting interactions, cont.
========================================================

```{r int_viz2,echo=FALSE,fig.height=9,fig.width=10}
p + geom_smooth(se = FALSE, method = "lm", fullrange = TRUE) + xlim(c(0, 140))
```

Interpretting interactions
========================================================

You could interpret the interaction as two regression equations:

$$
\begin{align}
  no &h.s.: \widehat{kid score} &= -11 + 51 \cdot 0 + 1.1 \cdot mom\,IQ\\
  & & -\;0.5 \cdot 0 \cdot mom\,IQ\\
  &h.s.: \widehat{kid score} &= -11 + 51 \cdot 1 + 1.1 \cdot mom\,IQ\\
  & & -\;0.5 \cdot 1 \cdot mom\,IQ
\end{align}
$$

Other things:
* interaction interpretability is improved by pre-processing (tomorrow)
* interactions help data models fit different subsets of data well (last few classes)

Statistical inference
========================================================

Units
---

People or schools, not pounds or inches, (the "other" units matter, though!)

Outcome and predictors
---

The thing we're predicting.
We don't talk about independent and dependent variables, those words are already used for other things.
Same goes for predictors instead of independent variables.

Inputs
========================================================

That last regression has:

* four predictors
  * intercept
  * mom HS
  * mom IQ
  * mom HS $\cdot$ mom IQ
* two inputs
  * mom's IQ
  * mom's high school status

Residuals
========================================================

Residuals represent the uncertainty left after removing predicted variance.

$$
r_i = y_i - \hat{y}_i
$$

```{r resid_graph,echo=FALSE,fig.height=6,fig.width=9}
x <- seq(-80, 80, by = .01)
my.lm4 <- lm(kid_score ~ 1, data = df)
a <- data.frame(x = rep(x, 5), density = c(dnorm(x, sd = sum(df$kid_score^2)/length(df$kid_score - 1)), dnorm(x, mean = 0, sd = sd(residuals(my.lm4))), dnorm(x, mean = 0, sd = sd(residuals(my.lm))), dnorm(x, mean = 0, sd = sd(residuals(my.lm2))), dnorm(x, mean = 0, sd = sd(residuals(my.lm3)))), group = factor(rep(0:4, each = length(x)), labels = c("null", "int", "hs", "iq+hs", "iq*hs")))
qplot(x, density, data = a, geom = "line", color = group, alpha = .5) + theme_bw(base_size = 24) + guides(alpha = FALSE)
```

Two kinds of uncertainty
========================================================

coefficient SE
-----

This expresses uncertainty for each coefficient. This is independent of residual variability!

residual variance
-----

* remaining variance is uncertainty in your outcome
* original variance - error variance is your explained error as a proportion
* use this to know how far off our estimates should be
* Adjusted $R^2$ is generally the same as explained error with a penalty for the number of parameters

Prediction and validation
========================================================
left: 50%

**"prediction" standard errors**

```{r predict,echo=FALSE}
x.new <- data.frame(mom_hs=1, mom_iq=100)
```

```{r predict_code}
predict(my.lm1, x.new, interval="confidence", level=0.95)
predict(my.lm1, x.new, interval="prediction", level=0.95)
```

***

**split-half cross-validation**

* Fit model to a random half of the dataset, check fit to remaining half.
* related to bootstrapping and simulation.

**external validation**

Fitting the same model to a new set of data.

**bootstrap/simulation**

We'll be covering this Friday.

Assumptions and diagnostics
========================================================

In order of importance:

1. Validity
2. Additivity & Linearity
3. Independence of errors
4. Equal variance of errors
5. Normality of errors