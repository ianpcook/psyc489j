<script type="text/javascript"
       src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
   tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
 });
</script>

Markov chains
========================================================
author: Jeffrey Chrabaszcz
date: 14 January 2014
transition: none
width: 1024
height: 760

```{r setup, include=FALSE, echo=FALSE}
require(knitr)
require(ggplot2)
opts_chunk$set(include=TRUE,cache=TRUE,fig.align='center')
options(digits = 3)
```

Outline
========================================================

1. Fixing a mistake
2. Likelihood calculation for continuous variables
3. Maximizing likelihood
4. Markov chains

Fixing a mistake
========================================================

Yesterday I listed the objective function as:

$$
\hat{\ell}(\theta|x) = \frac{1}{n}\sum_{i = 1}^n \ln f(x_i|\theta)
$$

But had you calculate a likelihood using:

```{r,eval=FALSE}
sum(dpois(obs, rate))
```

This code is wrong! Fixing it might yield some insight into how likelihood calculation works.

Unions
========================================================

We need two pieces of information to make this work.
The first is:

$$
P(A \cup B) = P(A) \cdot P(B)
$$

The second is that:

$$
log(A \cdot B) = log(A) + log(B)
$$

We can think of the objective function as the joint probability of each piece of data having been observed, given the parameter/hypothesis.

Correcting our code
========================================================

This means that our code should have read:

```{r,eval=FALSE}
prod(dpois(obs, rate))
```

Unfortunately, this can lead to values so small that it's hard for computers to easily keep track of them, (I'm not kidding).
Instead, we use the form:

```{r,eval=FALSE}
exp(sum(log(dpois(obs, rate))))
```

We take a natural logarithm so we can add values together, then exponentiate so we can view the result on a standard scale.

Likelihood calculation for continuous variable
========================================================

The normal distribution is defined for all possible values of X.
Say we sample five values from a standard normal distribution.

```{r rand_print,echo=FALSE}
n <- 5
dat <- rnorm(n)
dat
```

We can visualize the probability density associated with each of these samples assuming $\mu = 0$ and $\sigma = 1$ on the following graph.

```{r lik_plot,echo=FALSE,fig.height=4,fig.width=10}
df <- data.frame(x = rep(dat,2), y = c(dnorm(dat), rep(0, n)), groups = rep(1:n, 2))
df.2 <- data.frame(x = seq(-5, 5, by = .01), y = dnorm(seq(-5, 5, by = .01)))
ggplot(df, aes(x = x, y = y, group = groups)) + geom_line(color = "firebrick") + geom_line(data = df.2, aes(x = x, y = y, group = 1), color = "black") + theme_grey(base_size = 24) + xlim(c(-5, 5))
```

Heights of lines
========================================================

The heights of those lines are dictated by the parameters of the normal distribution.
That means we can change the heights by changing the parameters!

```{r lik_plot2,echo=FALSE,fig.height=7,fig.width=14}
df <- data.frame(x = rep(dat,4), y = c(dnorm(dat), rep(0, n), dnorm(dat, mean = 1), rep(0, n)), groups = rep(1:n, 4), mu = rep(0:1, each = 2*n))
df.2 <- data.frame(x = rep(seq(-5, 5, by = .01), 2), y = c(dnorm(seq(-5, 5, by = .01)), dnorm(seq(-5, 5, by = .01), mean = 1)), mu = rep(0:1, each = length(seq(-5, 5, by = .01))))
ggplot(df, aes(x = x, y = y, group = groups)) + geom_line(color = "firebrick") + geom_line(data = df.2, aes(x = x, y = y, group = 1), color = "black") + theme_grey(base_size = 24) + facet_wrap(~mu) + xlim(c(-5, 5))
```

Maximizing likelihood
========================================================

```{r}
logL <- function(coefs, y, x, sdr) {
  means <- coefs[1] + coefs[2] * x
  dat <- cbind(y, means, sdr)
  ll <- apply(dat, 1, function(x) dnorm(x[1], mean = x[2], sd = x[3]))
  return(sum(log(ll)))
}
bootS <- function(model.formula, df) {
  new.df <- df[sample(nrow(df), nrow(df), replace = TRUE),]
  mod <- lm(model.formula, data = new.df)
  return(coef(mod))
}
getRSD <- function(coefs, y, x) {
  y.hat <- coefs[1] + coefs[2] * x
  resids <- y - y.hat
  return(sd(resids))
}
```

Outputs for ML (lm vs. bootstrap)
========================================================

```{r,echo=FALSE}
options(digits = 5)
```

```{r}
data(women)
fit <- lm(weight ~ height, data = women)
fit.ll <- logL(coef(fit), women$weight, women$height, sd(residuals(fit)))
b <- t(replicate(100, bootS(weight ~ height, women)))
rsds <- apply(b, 1, function(x) getRSD(x, women$weight, women$height))
a <- cbind(b, rsds)
all.lls <- apply(a, 1, function(x) logL(x[1:2], women$weight, women$height, x[3]))
c(fit.ll, max(all.lls))
```

```{r,echo=FALSE}
options(digits = 3)
```

Random search
========================================================

A class of optimization methods that can be used with problems that are not continuous or differentiable.
Encompasses two general approaches.

Point estimation
---

Find a maximum/minimum point for some outcome.
A lot of you probably think of information criteria in this way.

Distribution estimation
---

A sampling procedure that converges to a distribution rather than a single point.
This is what we'll be doing.

Quick point-estimation example
========================================================

Say we have a normal distribution and need to find the x value for which f(x) is maximized.
We'll set up a toy example for which the true maximized value is 0.

```{r}
xs <- runif(10000, -3, 10)
xs[dnorm(xs) == max(dnorm(xs))]
```

I can't honest recommend this method, but it's a very simple demonstration of how random search could work.

Markov chains
========================================================

Kruschke's island-hopping example is the clearest explanation I've ever heard for this.

========================================================

```{r mc_code}
islands <- 1:6
visits <- rep(NA, times = 100000)
visits[1] <- sample(islands, 1)
for (i in 1:(length(visits) - 1)) {
  if (visits[i] == 1) {
    choose.direction <- sample(c(0, 1), 1)
  } else if (visits[i] == 6) {
    choose.direction <- sample(c(-1, 0), 1)
  } else {
    choose.direction <- sample(c(-1, 1), 1)
  }
  if (islands[visits[i]] < islands[visits[i] + choose.direction]) {
    visits[i + 1] <- visits[i] + choose.direction
  } else if (runif(1) < islands[visits[i] + choose.direction]/islands[visits[i]]) {
    visits[i + 1] <- islands[visits[i]] + choose.direction
  } else {
    visits[i + 1] <- visits[i]
  }
}
```

Island example output
========================================================

```{r run_mc}
mcmc.approx <- table(visits)/length(visits)
true.props <- islands/sum(islands)
names(true.props) <- names(mcmc.approx)

mcmc.approx
true.props
```